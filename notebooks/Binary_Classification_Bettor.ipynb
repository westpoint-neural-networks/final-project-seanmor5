{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Classification Bettor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFE6a9Sf-Fow",
        "colab_type": "code",
        "outputId": "eed50161-142c-4202-f998-0117b258996b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blkgwws7THid",
        "colab_type": "text"
      },
      "source": [
        "# Get Training and Validation Sets\n",
        "\n",
        "Here we gather a random sample of 2/3rds of the data to create both a training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqcfHE90fEDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = pd.read_csv('gdrive/My Drive/sbgan/training_data.csv', index_col=0).reset_index(drop=True)\n",
        "val_set = pd.read_csv('gdrive/My Drive/sbgan/validation_data.csv', index_col=0).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2YbYCAeUFZf",
        "colab_type": "text"
      },
      "source": [
        "# Extract Appropriate Data\n",
        "\n",
        "Extract the fields we want from training and validation sets. We also need to get the odds to calculate profitability in the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17yY1oLqft5E",
        "colab_type": "code",
        "outputId": "7dea36bc-fcc6-4b9f-d6e0-32f4a0edc115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "X_train = train_set.drop(['ODDS', 'OUTCOME'], axis=1)\n",
        "y_train = train_set['OUTCOME']\n",
        "\n",
        "X_val = val_set.drop(['ODDS', 'OUTCOME'], axis=1)\n",
        "y_val = val_set['OUTCOME']\n",
        "\n",
        "odds = val_set['ODDS']\n",
        "\n",
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WIN%_WINNER</th>\n",
              "      <th>PTS_WINNER</th>\n",
              "      <th>FGM_WINNER</th>\n",
              "      <th>FGA_WINNER</th>\n",
              "      <th>FG%_WINNER</th>\n",
              "      <th>3PM_WINNER</th>\n",
              "      <th>3PA_WINNER</th>\n",
              "      <th>3P%_WINNER</th>\n",
              "      <th>FTM_WINNER</th>\n",
              "      <th>FTA_WINNER</th>\n",
              "      <th>FT%_WINNER</th>\n",
              "      <th>OREB_WINNER</th>\n",
              "      <th>DREB_WINNER</th>\n",
              "      <th>AST_WINNER</th>\n",
              "      <th>TOV_WINNER</th>\n",
              "      <th>STL_WINNER</th>\n",
              "      <th>BLK_WINNER</th>\n",
              "      <th>BLKA_WINNER</th>\n",
              "      <th>PF_WINNER</th>\n",
              "      <th>PFD_WINNER</th>\n",
              "      <th>+/-_WINNER</th>\n",
              "      <th>OFFRTG_WINNER</th>\n",
              "      <th>DEFRTG_WINNER</th>\n",
              "      <th>NETRTG_WINNER</th>\n",
              "      <th>AST%_WINNER</th>\n",
              "      <th>AST/TO_WINNER</th>\n",
              "      <th>ASTRATIO_WINNER</th>\n",
              "      <th>OREB%_WINNER</th>\n",
              "      <th>DREB%_WINNER</th>\n",
              "      <th>REB%_WINNER</th>\n",
              "      <th>TOV%_WINNER</th>\n",
              "      <th>EFG%_WINNER</th>\n",
              "      <th>TS%_WINNER</th>\n",
              "      <th>PACE_WINNER</th>\n",
              "      <th>PIE_WINNER</th>\n",
              "      <th>FTARATE_WINNER</th>\n",
              "      <th>OPPEFG%_WINNER</th>\n",
              "      <th>OPPFTARATE_WINNER</th>\n",
              "      <th>OPPTOV%_WINNER</th>\n",
              "      <th>OPPOREB%_WINNER</th>\n",
              "      <th>...</th>\n",
              "      <th>OPPPITP_LOSER</th>\n",
              "      <th>%FGA2PT_LOSER</th>\n",
              "      <th>%FGA3PT_LOSER</th>\n",
              "      <th>%PTS2PT_LOSER</th>\n",
              "      <th>%PTS2PT-MR_LOSER</th>\n",
              "      <th>%PTS3PT_LOSER</th>\n",
              "      <th>%PTSFBPS_LOSER</th>\n",
              "      <th>%PTSFT_LOSER</th>\n",
              "      <th>%PTSOFFTO_LOSER</th>\n",
              "      <th>%PTSPITP_LOSER</th>\n",
              "      <th>2FGM%AST_LOSER</th>\n",
              "      <th>2FGM%UAST_LOSER</th>\n",
              "      <th>3FGM%AST_LOSER</th>\n",
              "      <th>3FGM%UAST_LOSER</th>\n",
              "      <th>FGM%AST_LOSER</th>\n",
              "      <th>FGM%UAST_LOSER</th>\n",
              "      <th>OPPFGM_LOSER</th>\n",
              "      <th>OPPFGA_LOSER</th>\n",
              "      <th>OPPFG%_LOSER</th>\n",
              "      <th>OPP3PM_LOSER</th>\n",
              "      <th>OPP3PA_LOSER</th>\n",
              "      <th>OPP3P%_LOSER</th>\n",
              "      <th>OPPFTM_LOSER</th>\n",
              "      <th>OPPFTA_LOSER</th>\n",
              "      <th>OPPFT%_LOSER</th>\n",
              "      <th>OPPOREB_LOSER</th>\n",
              "      <th>OPPDREB_LOSER</th>\n",
              "      <th>OPPREB_LOSER</th>\n",
              "      <th>OPPAST_LOSER</th>\n",
              "      <th>OPPTOV_LOSER</th>\n",
              "      <th>OPPSTL_LOSER</th>\n",
              "      <th>OPPBLK_LOSER</th>\n",
              "      <th>OPPBLKA_LOSER</th>\n",
              "      <th>OPPPF_LOSER</th>\n",
              "      <th>OPPPFD_LOSER</th>\n",
              "      <th>OPPPTS_LOSER</th>\n",
              "      <th>OPP+/-_LOSER</th>\n",
              "      <th>OPPPTS2NDCHANCE_LOSER</th>\n",
              "      <th>OPPPTSFB_LOSER</th>\n",
              "      <th>OPPPTSPAINT_LOSER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.444</td>\n",
              "      <td>-0.977860</td>\n",
              "      <td>-1.536727</td>\n",
              "      <td>-0.845358</td>\n",
              "      <td>0.429</td>\n",
              "      <td>-0.482446</td>\n",
              "      <td>0.016104</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.807257</td>\n",
              "      <td>0.666005</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.112221</td>\n",
              "      <td>-0.612435</td>\n",
              "      <td>-1.401929</td>\n",
              "      <td>1.384743</td>\n",
              "      <td>1.782603</td>\n",
              "      <td>-0.766028</td>\n",
              "      <td>0.187384</td>\n",
              "      <td>0.819961</td>\n",
              "      <td>1.279438</td>\n",
              "      <td>0.132884</td>\n",
              "      <td>-1.389504</td>\n",
              "      <td>-1.709050</td>\n",
              "      <td>0.140708</td>\n",
              "      <td>0.547</td>\n",
              "      <td>-1.782645</td>\n",
              "      <td>-1.512716</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.471</td>\n",
              "      <td>0.521</td>\n",
              "      <td>-0.264547</td>\n",
              "      <td>0.055776</td>\n",
              "      <td>0.869978</td>\n",
              "      <td>0.486</td>\n",
              "      <td>0.895727</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.286</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.688851</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.537</td>\n",
              "      <td>0.831</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.629332</td>\n",
              "      <td>-0.015150</td>\n",
              "      <td>0.436</td>\n",
              "      <td>-1.016631</td>\n",
              "      <td>-0.998663</td>\n",
              "      <td>0.351</td>\n",
              "      <td>-0.787086</td>\n",
              "      <td>-0.651989</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.584581</td>\n",
              "      <td>-0.377488</td>\n",
              "      <td>-0.094579</td>\n",
              "      <td>-0.771560</td>\n",
              "      <td>0.102330</td>\n",
              "      <td>1.236210</td>\n",
              "      <td>1.092759</td>\n",
              "      <td>-1.115536</td>\n",
              "      <td>1.160450</td>\n",
              "      <td>-0.937112</td>\n",
              "      <td>-1.055549</td>\n",
              "      <td>-0.238085</td>\n",
              "      <td>-0.287141</td>\n",
              "      <td>1.566566</td>\n",
              "      <td>-0.688851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.200</td>\n",
              "      <td>0.701097</td>\n",
              "      <td>0.949025</td>\n",
              "      <td>0.994177</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.714700</td>\n",
              "      <td>1.049039</td>\n",
              "      <td>0.333</td>\n",
              "      <td>-0.723071</td>\n",
              "      <td>-0.897560</td>\n",
              "      <td>0.776</td>\n",
              "      <td>-1.340156</td>\n",
              "      <td>-0.492926</td>\n",
              "      <td>0.869716</td>\n",
              "      <td>0.910959</td>\n",
              "      <td>1.478103</td>\n",
              "      <td>0.137509</td>\n",
              "      <td>-0.175947</td>\n",
              "      <td>1.867855</td>\n",
              "      <td>-0.525978</td>\n",
              "      <td>-1.828037</td>\n",
              "      <td>-0.291588</td>\n",
              "      <td>2.131022</td>\n",
              "      <td>-1.695006</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.033760</td>\n",
              "      <td>0.508716</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.551</td>\n",
              "      <td>1.420324</td>\n",
              "      <td>-1.354220</td>\n",
              "      <td>-1.076329</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.681684</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.313</td>\n",
              "      <td>...</td>\n",
              "      <td>1.047926</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.635</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.769692</td>\n",
              "      <td>0.928952</td>\n",
              "      <td>0.455</td>\n",
              "      <td>1.044930</td>\n",
              "      <td>0.990900</td>\n",
              "      <td>0.363</td>\n",
              "      <td>-0.640025</td>\n",
              "      <td>-0.613616</td>\n",
              "      <td>0.756</td>\n",
              "      <td>-1.839417</td>\n",
              "      <td>0.868134</td>\n",
              "      <td>-0.008498</td>\n",
              "      <td>-0.465953</td>\n",
              "      <td>-0.048169</td>\n",
              "      <td>1.236210</td>\n",
              "      <td>-0.208563</td>\n",
              "      <td>0.746932</td>\n",
              "      <td>1.160450</td>\n",
              "      <td>0.165235</td>\n",
              "      <td>0.703066</td>\n",
              "      <td>-0.545638</td>\n",
              "      <td>-1.281381</td>\n",
              "      <td>-1.245284</td>\n",
              "      <td>1.047926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.677</td>\n",
              "      <td>1.271018</td>\n",
              "      <td>1.481686</td>\n",
              "      <td>0.807558</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.012924</td>\n",
              "      <td>0.046938</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.760883</td>\n",
              "      <td>0.447833</td>\n",
              "      <td>0.789</td>\n",
              "      <td>1.491980</td>\n",
              "      <td>1.140365</td>\n",
              "      <td>0.335211</td>\n",
              "      <td>0.753031</td>\n",
              "      <td>-0.348897</td>\n",
              "      <td>1.379871</td>\n",
              "      <td>-0.418167</td>\n",
              "      <td>-0.166291</td>\n",
              "      <td>-0.525978</td>\n",
              "      <td>1.245330</td>\n",
              "      <td>1.554907</td>\n",
              "      <td>-0.167613</td>\n",
              "      <td>1.283346</td>\n",
              "      <td>0.551</td>\n",
              "      <td>-0.218518</td>\n",
              "      <td>-0.035516</td>\n",
              "      <td>0.339</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.564301</td>\n",
              "      <td>1.199734</td>\n",
              "      <td>0.065505</td>\n",
              "      <td>0.487</td>\n",
              "      <td>-0.415285</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.284</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072476</td>\n",
              "      <td>0.808</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.656</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.899</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.603</td>\n",
              "      <td>0.397</td>\n",
              "      <td>-0.192137</td>\n",
              "      <td>-0.066182</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.642675</td>\n",
              "      <td>0.674801</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.536463</td>\n",
              "      <td>0.537583</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.957504</td>\n",
              "      <td>-0.288515</td>\n",
              "      <td>0.163664</td>\n",
              "      <td>1.011146</td>\n",
              "      <td>0.553829</td>\n",
              "      <td>0.999838</td>\n",
              "      <td>0.501249</td>\n",
              "      <td>0.863337</td>\n",
              "      <td>-0.584437</td>\n",
              "      <td>0.471442</td>\n",
              "      <td>0.255419</td>\n",
              "      <td>0.723019</td>\n",
              "      <td>0.860058</td>\n",
              "      <td>0.264784</td>\n",
              "      <td>0.072476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.600</td>\n",
              "      <td>1.579083</td>\n",
              "      <td>1.614851</td>\n",
              "      <td>1.820635</td>\n",
              "      <td>0.459</td>\n",
              "      <td>2.118251</td>\n",
              "      <td>2.051141</td>\n",
              "      <td>0.367</td>\n",
              "      <td>-1.001313</td>\n",
              "      <td>-1.406627</td>\n",
              "      <td>0.797</td>\n",
              "      <td>-0.396111</td>\n",
              "      <td>1.100529</td>\n",
              "      <td>1.849640</td>\n",
              "      <td>-1.063141</td>\n",
              "      <td>0.970603</td>\n",
              "      <td>0.589277</td>\n",
              "      <td>-1.387048</td>\n",
              "      <td>0.080272</td>\n",
              "      <td>-0.525978</td>\n",
              "      <td>1.113345</td>\n",
              "      <td>1.330333</td>\n",
              "      <td>-0.167613</td>\n",
              "      <td>1.133492</td>\n",
              "      <td>0.629</td>\n",
              "      <td>2.354722</td>\n",
              "      <td>1.519432</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.566</td>\n",
              "      <td>1.230096</td>\n",
              "      <td>1.040112</td>\n",
              "      <td>-1.673197</td>\n",
              "      <td>0.506</td>\n",
              "      <td>-0.227998</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.267</td>\n",
              "      <td>...</td>\n",
              "      <td>1.571338</td>\n",
              "      <td>0.619</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.526</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.454</td>\n",
              "      <td>0.837</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.371</td>\n",
              "      <td>1.993838</td>\n",
              "      <td>1.541341</td>\n",
              "      <td>0.473</td>\n",
              "      <td>2.050570</td>\n",
              "      <td>1.976384</td>\n",
              "      <td>0.364</td>\n",
              "      <td>1.467849</td>\n",
              "      <td>1.765529</td>\n",
              "      <td>0.744</td>\n",
              "      <td>-0.254495</td>\n",
              "      <td>1.179539</td>\n",
              "      <td>0.981431</td>\n",
              "      <td>2.793852</td>\n",
              "      <td>1.155826</td>\n",
              "      <td>4.072673</td>\n",
              "      <td>0.382947</td>\n",
              "      <td>0.514124</td>\n",
              "      <td>1.289701</td>\n",
              "      <td>1.879996</td>\n",
              "      <td>2.573593</td>\n",
              "      <td>1.491902</td>\n",
              "      <td>1.089498</td>\n",
              "      <td>2.451778</td>\n",
              "      <td>1.571338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.694</td>\n",
              "      <td>1.933359</td>\n",
              "      <td>1.170967</td>\n",
              "      <td>0.860878</td>\n",
              "      <td>0.468</td>\n",
              "      <td>2.696183</td>\n",
              "      <td>2.621568</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.343521</td>\n",
              "      <td>0.338747</td>\n",
              "      <td>0.760</td>\n",
              "      <td>-0.033016</td>\n",
              "      <td>0.343638</td>\n",
              "      <td>1.671472</td>\n",
              "      <td>0.674067</td>\n",
              "      <td>0.564603</td>\n",
              "      <td>-0.314260</td>\n",
              "      <td>-0.054836</td>\n",
              "      <td>-0.659418</td>\n",
              "      <td>-0.525978</td>\n",
              "      <td>1.151055</td>\n",
              "      <td>2.228628</td>\n",
              "      <td>0.670713</td>\n",
              "      <td>1.208419</td>\n",
              "      <td>0.633</td>\n",
              "      <td>0.790596</td>\n",
              "      <td>1.441685</td>\n",
              "      <td>0.288</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.586</td>\n",
              "      <td>1.023564</td>\n",
              "      <td>0.694264</td>\n",
              "      <td>-0.012348</td>\n",
              "      <td>0.517</td>\n",
              "      <td>-0.736350</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.287</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.636</td>\n",
              "      <td>0.364</td>\n",
              "      <td>0.332497</td>\n",
              "      <td>0.597240</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.542111</td>\n",
              "      <td>0.544643</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.340382</td>\n",
              "      <td>0.537583</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.118427</td>\n",
              "      <td>0.779161</td>\n",
              "      <td>0.809270</td>\n",
              "      <td>-0.109412</td>\n",
              "      <td>-0.123419</td>\n",
              "      <td>0.999838</td>\n",
              "      <td>0.028041</td>\n",
              "      <td>0.979741</td>\n",
              "      <td>-0.519811</td>\n",
              "      <td>0.593925</td>\n",
              "      <td>0.527205</td>\n",
              "      <td>0.953684</td>\n",
              "      <td>0.095258</td>\n",
              "      <td>1.358281</td>\n",
              "      <td>0.001101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17626</th>\n",
              "      <td>0.529</td>\n",
              "      <td>0.531661</td>\n",
              "      <td>0.505141</td>\n",
              "      <td>-0.445459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.260610</td>\n",
              "      <td>0.278192</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.250774</td>\n",
              "      <td>0.047852</td>\n",
              "      <td>0.775</td>\n",
              "      <td>-1.848488</td>\n",
              "      <td>-0.333581</td>\n",
              "      <td>0.246127</td>\n",
              "      <td>-0.510393</td>\n",
              "      <td>0.869103</td>\n",
              "      <td>0.363393</td>\n",
              "      <td>-0.781497</td>\n",
              "      <td>0.450117</td>\n",
              "      <td>-0.525978</td>\n",
              "      <td>-0.093376</td>\n",
              "      <td>0.706517</td>\n",
              "      <td>0.805927</td>\n",
              "      <td>-0.027878</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.538317</td>\n",
              "      <td>0.508716</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.292547</td>\n",
              "      <td>0.055776</td>\n",
              "      <td>0.169308</td>\n",
              "      <td>0.526</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.287</td>\n",
              "      <td>...</td>\n",
              "      <td>0.476930</td>\n",
              "      <td>0.643</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.868</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.623</td>\n",
              "      <td>0.377</td>\n",
              "      <td>1.250607</td>\n",
              "      <td>0.648273</td>\n",
              "      <td>0.473</td>\n",
              "      <td>1.799160</td>\n",
              "      <td>1.455751</td>\n",
              "      <td>0.381</td>\n",
              "      <td>-0.934147</td>\n",
              "      <td>-1.035722</td>\n",
              "      <td>0.766</td>\n",
              "      <td>-0.440957</td>\n",
              "      <td>0.556728</td>\n",
              "      <td>0.335825</td>\n",
              "      <td>2.080769</td>\n",
              "      <td>0.704328</td>\n",
              "      <td>1.236210</td>\n",
              "      <td>0.382947</td>\n",
              "      <td>-0.649919</td>\n",
              "      <td>-0.002808</td>\n",
              "      <td>-0.569663</td>\n",
              "      <td>1.166701</td>\n",
              "      <td>0.972906</td>\n",
              "      <td>0.095258</td>\n",
              "      <td>0.108570</td>\n",
              "      <td>0.476930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17627</th>\n",
              "      <td>0.463</td>\n",
              "      <td>-0.022857</td>\n",
              "      <td>-0.338239</td>\n",
              "      <td>-0.578758</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.178048</td>\n",
              "      <td>0.231941</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.436268</td>\n",
              "      <td>1.284159</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.693172</td>\n",
              "      <td>0.303801</td>\n",
              "      <td>0.112501</td>\n",
              "      <td>0.516139</td>\n",
              "      <td>-0.450397</td>\n",
              "      <td>0.476335</td>\n",
              "      <td>0.187384</td>\n",
              "      <td>-1.460748</td>\n",
              "      <td>1.343917</td>\n",
              "      <td>0.095174</td>\n",
              "      <td>0.232417</td>\n",
              "      <td>0.021687</td>\n",
              "      <td>0.178171</td>\n",
              "      <td>0.598</td>\n",
              "      <td>-0.268974</td>\n",
              "      <td>0.119979</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.696</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.545</td>\n",
              "      <td>-0.066167</td>\n",
              "      <td>0.321812</td>\n",
              "      <td>1.285191</td>\n",
              "      <td>0.494</td>\n",
              "      <td>-1.298212</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.304</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070273</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.366</td>\n",
              "      <td>-0.935368</td>\n",
              "      <td>-0.576507</td>\n",
              "      <td>0.440</td>\n",
              "      <td>-0.614375</td>\n",
              "      <td>-0.515218</td>\n",
              "      <td>0.343</td>\n",
              "      <td>-0.247863</td>\n",
              "      <td>0.230597</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.491350</td>\n",
              "      <td>0.111863</td>\n",
              "      <td>0.335825</td>\n",
              "      <td>-0.364084</td>\n",
              "      <td>0.854828</td>\n",
              "      <td>0.172536</td>\n",
              "      <td>-0.208563</td>\n",
              "      <td>-0.882727</td>\n",
              "      <td>-0.584437</td>\n",
              "      <td>0.471442</td>\n",
              "      <td>-0.943637</td>\n",
              "      <td>-0.007420</td>\n",
              "      <td>0.860058</td>\n",
              "      <td>-0.099715</td>\n",
              "      <td>-0.070273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17628</th>\n",
              "      <td>0.154</td>\n",
              "      <td>-2.240929</td>\n",
              "      <td>-1.581115</td>\n",
              "      <td>-1.458536</td>\n",
              "      <td>0.440</td>\n",
              "      <td>-1.844716</td>\n",
              "      <td>-1.648926</td>\n",
              "      <td>0.298</td>\n",
              "      <td>-1.418675</td>\n",
              "      <td>-1.333903</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.475316</td>\n",
              "      <td>-0.891290</td>\n",
              "      <td>-1.179218</td>\n",
              "      <td>0.437175</td>\n",
              "      <td>-0.450397</td>\n",
              "      <td>0.024567</td>\n",
              "      <td>0.429604</td>\n",
              "      <td>-0.227932</td>\n",
              "      <td>-1.815561</td>\n",
              "      <td>-1.205821</td>\n",
              "      <td>-2.038273</td>\n",
              "      <td>-0.356912</td>\n",
              "      <td>-1.282907</td>\n",
              "      <td>0.563</td>\n",
              "      <td>-1.177177</td>\n",
              "      <td>-0.657495</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.502</td>\n",
              "      <td>-1.737450</td>\n",
              "      <td>-1.061579</td>\n",
              "      <td>-0.713018</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.494397</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.282</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.641268</td>\n",
              "      <td>0.768</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.411</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.471</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.406</td>\n",
              "      <td>-1.328844</td>\n",
              "      <td>-0.933735</td>\n",
              "      <td>0.436</td>\n",
              "      <td>-1.318323</td>\n",
              "      <td>-0.905693</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-0.051781</td>\n",
              "      <td>0.192223</td>\n",
              "      <td>0.742</td>\n",
              "      <td>-0.254495</td>\n",
              "      <td>-0.155055</td>\n",
              "      <td>-0.266740</td>\n",
              "      <td>-1.331839</td>\n",
              "      <td>1.607325</td>\n",
              "      <td>-0.182022</td>\n",
              "      <td>0.264645</td>\n",
              "      <td>0.281315</td>\n",
              "      <td>0.320319</td>\n",
              "      <td>-0.202214</td>\n",
              "      <td>-1.407272</td>\n",
              "      <td>-1.103079</td>\n",
              "      <td>-0.210661</td>\n",
              "      <td>1.202067</td>\n",
              "      <td>-0.641268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17629</th>\n",
              "      <td>0.242</td>\n",
              "      <td>-0.685198</td>\n",
              "      <td>-0.027520</td>\n",
              "      <td>0.434319</td>\n",
              "      <td>0.445</td>\n",
              "      <td>-1.679593</td>\n",
              "      <td>-1.664343</td>\n",
              "      <td>0.338</td>\n",
              "      <td>-0.166588</td>\n",
              "      <td>-0.279406</td>\n",
              "      <td>0.768</td>\n",
              "      <td>0.765791</td>\n",
              "      <td>0.343638</td>\n",
              "      <td>-0.154751</td>\n",
              "      <td>1.463707</td>\n",
              "      <td>-1.363898</td>\n",
              "      <td>0.250451</td>\n",
              "      <td>0.550714</td>\n",
              "      <td>0.080272</td>\n",
              "      <td>-0.654936</td>\n",
              "      <td>-1.545212</td>\n",
              "      <td>-1.539220</td>\n",
              "      <td>0.427328</td>\n",
              "      <td>-1.470225</td>\n",
              "      <td>0.570</td>\n",
              "      <td>-1.025809</td>\n",
              "      <td>-0.424253</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.469</td>\n",
              "      <td>0.511</td>\n",
              "      <td>0.523538</td>\n",
              "      <td>-1.141390</td>\n",
              "      <td>-0.401609</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.280354</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.306</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.213022</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.422</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.899</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.608</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.813412</td>\n",
              "      <td>1.184114</td>\n",
              "      <td>0.450</td>\n",
              "      <td>-0.212119</td>\n",
              "      <td>-0.347871</td>\n",
              "      <td>0.367</td>\n",
              "      <td>-0.100802</td>\n",
              "      <td>0.307343</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.796580</td>\n",
              "      <td>0.156350</td>\n",
              "      <td>0.981431</td>\n",
              "      <td>1.571425</td>\n",
              "      <td>1.456825</td>\n",
              "      <td>0.408908</td>\n",
              "      <td>0.028041</td>\n",
              "      <td>2.260188</td>\n",
              "      <td>2.000580</td>\n",
              "      <td>0.103993</td>\n",
              "      <td>0.495230</td>\n",
              "      <td>-0.641749</td>\n",
              "      <td>1.013018</td>\n",
              "      <td>0.264784</td>\n",
              "      <td>-0.213022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17630</th>\n",
              "      <td>0.717</td>\n",
              "      <td>0.362225</td>\n",
              "      <td>0.549529</td>\n",
              "      <td>-1.805115</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.028356</td>\n",
              "      <td>-0.168899</td>\n",
              "      <td>0.370</td>\n",
              "      <td>-0.027467</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.756</td>\n",
              "      <td>-2.574677</td>\n",
              "      <td>-1.090472</td>\n",
              "      <td>0.647005</td>\n",
              "      <td>0.437175</td>\n",
              "      <td>1.376603</td>\n",
              "      <td>-0.653086</td>\n",
              "      <td>-2.355928</td>\n",
              "      <td>-0.474495</td>\n",
              "      <td>-0.010145</td>\n",
              "      <td>0.981360</td>\n",
              "      <td>1.230523</td>\n",
              "      <td>-0.275784</td>\n",
              "      <td>1.114760</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.185127</td>\n",
              "      <td>1.441685</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.597</td>\n",
              "      <td>-0.612391</td>\n",
              "      <td>1.252941</td>\n",
              "      <td>0.636422</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.333865</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.307</td>\n",
              "      <td>...</td>\n",
              "      <td>0.215224</td>\n",
              "      <td>0.773</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.649</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.785</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.469</td>\n",
              "      <td>0.157619</td>\n",
              "      <td>-0.499958</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.039291</td>\n",
              "      <td>-0.106149</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.095280</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.770</td>\n",
              "      <td>-0.254495</td>\n",
              "      <td>-0.199542</td>\n",
              "      <td>-0.309780</td>\n",
              "      <td>0.858343</td>\n",
              "      <td>1.381576</td>\n",
              "      <td>1.236210</td>\n",
              "      <td>-0.445167</td>\n",
              "      <td>0.281315</td>\n",
              "      <td>0.061818</td>\n",
              "      <td>-0.202214</td>\n",
              "      <td>0.159494</td>\n",
              "      <td>0.492354</td>\n",
              "      <td>-0.057702</td>\n",
              "      <td>0.525140</td>\n",
              "      <td>0.215224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17631 rows × 174 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       WIN%_WINNER  PTS_WINNER  ...  OPPPTSFB_LOSER  OPPPTSPAINT_LOSER\n",
              "0            0.444   -0.977860  ...        1.566566          -0.688851\n",
              "1            0.200    0.701097  ...       -1.245284           1.047926\n",
              "2            0.677    1.271018  ...        0.264784           0.072476\n",
              "3            0.600    1.579083  ...        2.451778           1.571338\n",
              "4            0.694    1.933359  ...        1.358281           0.001101\n",
              "...            ...         ...  ...             ...                ...\n",
              "17626        0.529    0.531661  ...        0.108570           0.476930\n",
              "17627        0.463   -0.022857  ...       -0.099715          -0.070273\n",
              "17628        0.154   -2.240929  ...        1.202067          -0.641268\n",
              "17629        0.242   -0.685198  ...        0.264784          -0.213022\n",
              "17630        0.717    0.362225  ...        0.525140           0.215224\n",
              "\n",
              "[17631 rows x 174 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C1U0fQFUOG3",
        "colab_type": "text"
      },
      "source": [
        "# Define Profitability Metric\n",
        "\n",
        "We need a custom Keras `Callback` to track Profitability from Epoch to Epoch. This is a bit hacky so there might be a better solution.\n",
        "\n",
        "Profitability is calculated from the odds, wager, and outcome. Odds are in decimal format. Wager is calculated using Kelly's criterion for betting.\n",
        "\n",
        "Kelly's criterion is:\n",
        "\n",
        "```f* = (b*p - q) / b```\n",
        "\n",
        "Where...\n",
        "* f* is the percentage bet\n",
        "* b is `odds - 1`\n",
        "* p is probability of victory\n",
        "* q is `1 - p`\n",
        "\n",
        "The great thing about Kelly is that if the probability of victory is low enough, the bet will be negative. Therefore, when we return a wager, we just need to return `max(0, kelly)`. \n",
        "\n",
        "In this formula, we multiply `kelly` by `100` to represent \"units.\" A single unit is `1%` of your bankroll. This makes sense to people who bet.\n",
        "\n",
        "Additionally, some bets will be huge. We want to limit this so we'll limit bets to `15`, although it might be useful to set this higher. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf12rBL1VaaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import cvxopt as opt  \n",
        "from cvxopt import blas, solvers\n",
        "\n",
        "solvers.options['show_progress'] = False\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/13746/how-to-define-a-custom-performance-metric-in-keras\n",
        "class Profit(Callback):\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.fixed = []\n",
        "    self.kelly = []\n",
        "  \n",
        "  # Bet a fixed rate\n",
        "  def calculate_fixed(self, wager, predictions, odds, targets):\n",
        "    data = zip(predictions, odds, targets)\n",
        "    profits = []\n",
        "    wagers = []\n",
        "    for pred, bprime, outcome in data:\n",
        "      prof = 0\n",
        "      wagers.append(wager)\n",
        "      if pred == 1:\n",
        "        prof = wager * bprime if outcome == 1 else 0\n",
        "      profits.append(prof)\n",
        "    self.fixed.append(np.sum(profits) - np.sum(wagers))\n",
        "  \n",
        "  # Bet based on Kelly's criterion\n",
        "  def calculate_kelly(self, scores, odds, targets, max_wager=15):\n",
        "    data = zip(scores, odds, targets)\n",
        "    profits = []\n",
        "    wagers = []\n",
        "    for p, bprime, outcome in data:\n",
        "      b = bprime - 1\n",
        "      q = 1 - p\n",
        "      units = max(0, round(float(((b*p - q) / b) * max_wager)))\n",
        "      wagers.append(units)\n",
        "      prof = units * bprime if outcome == 1 else 0\n",
        "      profits.append(prof)\n",
        "    self.kelly.append(np.sum(profits) - np.sum(wagers))\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    scores = np.asarray(self.model.predict(self.validation_data[0]))\n",
        "    predictions = np.round(scores)\n",
        "    targets = self.validation_data[1]\n",
        "    odds = np.asarray(self.validation_data[2].values)\n",
        "    self.calculate_fixed(5, predictions, odds, targets)\n",
        "    self.calculate_kelly(scores, odds, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPqBr9XWG7a",
        "colab_type": "text"
      },
      "source": [
        "# Define and Train Model\n",
        "\n",
        "Model is a Dense Feed Forward Binary Classification network. Output is the probability that the proposed winner will actually win.\n",
        "\n",
        "It's kind of hard to understand what this means so basically:\n",
        "\n",
        "1. Every game has two lines\n",
        "2. Each point represents a line formatted like: `(proposed winner stats, proposed loser stats, odds)`\n",
        "3. Network will output probability that `proposed winner` ACTUALLY wins\n",
        "\n",
        "For some reason sigmoid works the best here. I'll need to do some more legitimate testing to find out if I can tune the model more. It seems like the accuracy limit is 67%.\n",
        "\n",
        "TODO: Try to hyperparameterize a little bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nxZS8rcjl8T",
        "colab_type": "code",
        "outputId": "d691ecca-d02d-4cd9-f719-ed61dad83de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=174, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "stop = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
        "profit = Profit()\n",
        "checkpoint = ModelCheckpoint('gdrive/My Drive/sbgan/models/binary.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    y_train, \n",
        "                    validation_data=(X_val, y_val, odds), \n",
        "                    epochs=40000, \n",
        "                    callbacks=[checkpoint, stop], \n",
        "                    batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.6670 - accuracy: 0.5953WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6646 - accuracy: 0.5998 - val_loss: 1.5569 - val_accuracy: 0.6411\n",
            "Epoch 2/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.6348 - accuracy: 0.6509WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6505 - val_loss: 1.5677 - val_accuracy: 0.6462\n",
            "Epoch 3/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.6337 - accuracy: 0.6526WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6531 - val_loss: 1.5464 - val_accuracy: 0.6451\n",
            "Epoch 4/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.6537WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6527 - val_loss: 1.5361 - val_accuracy: 0.6488\n",
            "Epoch 5/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.6274 - accuracy: 0.6521WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6523 - val_loss: 1.5317 - val_accuracy: 0.6454\n",
            "Epoch 6/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.6567WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6561 - val_loss: 1.5272 - val_accuracy: 0.6454\n",
            "Epoch 7/40000\n",
            "133/138 [===========================>..] - ETA: 0s - loss: 0.6230 - accuracy: 0.6560WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6553 - val_loss: 1.5392 - val_accuracy: 0.6510\n",
            "Epoch 8/40000\n",
            "122/138 [=========================>....] - ETA: 0s - loss: 0.6237 - accuracy: 0.6526WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6232 - accuracy: 0.6537 - val_loss: 1.5282 - val_accuracy: 0.6474\n",
            "Epoch 9/40000\n",
            "133/138 [===========================>..] - ETA: 0s - loss: 0.6214 - accuracy: 0.6531WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6216 - accuracy: 0.6529 - val_loss: 1.5299 - val_accuracy: 0.6488\n",
            "Epoch 10/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.6551WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6554 - val_loss: 1.5277 - val_accuracy: 0.6483\n",
            "Epoch 11/40000\n",
            "138/138 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.6548WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6215 - accuracy: 0.6548 - val_loss: 1.5353 - val_accuracy: 0.6485\n",
            "Epoch 12/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.6598WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6586 - val_loss: 1.5611 - val_accuracy: 0.6479\n",
            "Epoch 13/40000\n",
            "126/138 [==========================>...] - ETA: 0s - loss: 0.6161 - accuracy: 0.6573WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6555 - val_loss: 1.5221 - val_accuracy: 0.6381\n",
            "Epoch 14/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.6169 - accuracy: 0.6573WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6575 - val_loss: 1.5225 - val_accuracy: 0.6466\n",
            "Epoch 15/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.6178 - accuracy: 0.6562WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6561 - val_loss: 1.5278 - val_accuracy: 0.6481\n",
            "Epoch 16/40000\n",
            "125/138 [==========================>...] - ETA: 0s - loss: 0.6161 - accuracy: 0.6563WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6155 - accuracy: 0.6569 - val_loss: 1.5227 - val_accuracy: 0.6488\n",
            "Epoch 17/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.6162 - accuracy: 0.6577WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6156 - accuracy: 0.6583 - val_loss: 1.5336 - val_accuracy: 0.6495\n",
            "Epoch 18/40000\n",
            "127/138 [==========================>...] - ETA: 0s - loss: 0.6148 - accuracy: 0.6615WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6145 - accuracy: 0.6616 - val_loss: 1.5369 - val_accuracy: 0.6490\n",
            "Epoch 19/40000\n",
            "128/138 [==========================>...] - ETA: 0s - loss: 0.6146 - accuracy: 0.6607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6608 - val_loss: 1.5212 - val_accuracy: 0.6479\n",
            "Epoch 20/40000\n",
            "128/138 [==========================>...] - ETA: 0s - loss: 0.6134 - accuracy: 0.6592WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6129 - accuracy: 0.6599 - val_loss: 1.5247 - val_accuracy: 0.6486\n",
            "Epoch 21/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.6153 - accuracy: 0.6579WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6588 - val_loss: 1.5357 - val_accuracy: 0.6466\n",
            "Epoch 22/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.6143 - accuracy: 0.6591WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6590 - val_loss: 1.5430 - val_accuracy: 0.6517\n",
            "Epoch 23/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.6132 - accuracy: 0.6595WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6129 - accuracy: 0.6599 - val_loss: 1.5291 - val_accuracy: 0.6461\n",
            "Epoch 24/40000\n",
            "131/138 [===========================>..] - ETA: 0s - loss: 0.6112 - accuracy: 0.6618WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6106 - accuracy: 0.6624 - val_loss: 1.5293 - val_accuracy: 0.6495\n",
            "Epoch 25/40000\n",
            "137/138 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.6625WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6102 - accuracy: 0.6626 - val_loss: 1.5360 - val_accuracy: 0.6471\n",
            "Epoch 26/40000\n",
            "128/138 [==========================>...] - ETA: 0s - loss: 0.6110 - accuracy: 0.6576WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6578 - val_loss: 1.5445 - val_accuracy: 0.6471\n",
            "Epoch 27/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.6086 - accuracy: 0.6636WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.6625 - val_loss: 1.5306 - val_accuracy: 0.6444\n",
            "Epoch 28/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.6090 - accuracy: 0.6629WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6635 - val_loss: 1.5339 - val_accuracy: 0.6490\n",
            "Epoch 29/40000\n",
            "131/138 [===========================>..] - ETA: 0s - loss: 0.6054 - accuracy: 0.6651WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6071 - accuracy: 0.6642 - val_loss: 1.5399 - val_accuracy: 0.6493\n",
            "Epoch 30/40000\n",
            "137/138 [============================>.] - ETA: 0s - loss: 0.6072 - accuracy: 0.6662WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6078 - accuracy: 0.6655 - val_loss: 1.5308 - val_accuracy: 0.6476\n",
            "Epoch 31/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.6620WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6058 - accuracy: 0.6629 - val_loss: 1.5327 - val_accuracy: 0.6476\n",
            "Epoch 32/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6056 - accuracy: 0.6660WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6047 - accuracy: 0.6670 - val_loss: 1.5466 - val_accuracy: 0.6456\n",
            "Epoch 33/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.6044 - accuracy: 0.6671WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6058 - accuracy: 0.6660 - val_loss: 1.5305 - val_accuracy: 0.6434\n",
            "Epoch 34/40000\n",
            "131/138 [===========================>..] - ETA: 0s - loss: 0.6040 - accuracy: 0.6679WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6670 - val_loss: 1.5351 - val_accuracy: 0.6464\n",
            "Epoch 35/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.6642WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6645 - val_loss: 1.5323 - val_accuracy: 0.6434\n",
            "Epoch 36/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.6025 - accuracy: 0.6641WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6031 - accuracy: 0.6642 - val_loss: 1.5382 - val_accuracy: 0.6420\n",
            "Epoch 37/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.6004 - accuracy: 0.6697WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6003 - accuracy: 0.6701 - val_loss: 1.5373 - val_accuracy: 0.6434\n",
            "Epoch 38/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.6646WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6022 - accuracy: 0.6646 - val_loss: 1.5577 - val_accuracy: 0.6497\n",
            "Epoch 39/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.5996 - accuracy: 0.6671WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6013 - accuracy: 0.6662 - val_loss: 1.5499 - val_accuracy: 0.6413\n",
            "Epoch 40/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5992 - accuracy: 0.6698WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6000 - accuracy: 0.6689 - val_loss: 1.5486 - val_accuracy: 0.6464\n",
            "Epoch 41/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.5986 - accuracy: 0.6696WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5980 - accuracy: 0.6695 - val_loss: 1.5461 - val_accuracy: 0.6445\n",
            "Epoch 42/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.5989 - accuracy: 0.6699WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.5982 - accuracy: 0.6702 - val_loss: 1.5538 - val_accuracy: 0.6457\n",
            "Epoch 43/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.6693WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5967 - accuracy: 0.6695 - val_loss: 1.5436 - val_accuracy: 0.6405\n",
            "Epoch 44/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.5962 - accuracy: 0.6695WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5964 - accuracy: 0.6701 - val_loss: 1.5516 - val_accuracy: 0.6464\n",
            "Epoch 45/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.5956 - accuracy: 0.6705WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5955 - accuracy: 0.6703 - val_loss: 1.5492 - val_accuracy: 0.6405\n",
            "Epoch 46/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5945 - accuracy: 0.6676WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5943 - accuracy: 0.6680 - val_loss: 1.5469 - val_accuracy: 0.6439\n",
            "Epoch 47/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.6727WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5936 - accuracy: 0.6718 - val_loss: 1.5520 - val_accuracy: 0.6418\n",
            "Epoch 48/40000\n",
            "137/138 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.6740WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5928 - accuracy: 0.6741 - val_loss: 1.5429 - val_accuracy: 0.6422\n",
            "Epoch 49/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5902 - accuracy: 0.6710WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5906 - accuracy: 0.6711 - val_loss: 1.5476 - val_accuracy: 0.6428\n",
            "Epoch 50/40000\n",
            "137/138 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.6722WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5914 - accuracy: 0.6723 - val_loss: 1.5628 - val_accuracy: 0.6427\n",
            "Epoch 51/40000\n",
            "133/138 [===========================>..] - ETA: 0s - loss: 0.5919 - accuracy: 0.6749WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5923 - accuracy: 0.6747 - val_loss: 1.5555 - val_accuracy: 0.6435\n",
            "Epoch 52/40000\n",
            "136/138 [============================>.] - ETA: 0s - loss: 0.5881 - accuracy: 0.6749WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5890 - accuracy: 0.6742 - val_loss: 1.5509 - val_accuracy: 0.6403\n",
            "Epoch 53/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5879 - accuracy: 0.6780WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5877 - accuracy: 0.6775 - val_loss: 1.5525 - val_accuracy: 0.6425\n",
            "Epoch 54/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5858 - accuracy: 0.6783WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5862 - accuracy: 0.6785 - val_loss: 1.5588 - val_accuracy: 0.6417\n",
            "Epoch 55/40000\n",
            "133/138 [===========================>..] - ETA: 0s - loss: 0.5863 - accuracy: 0.6765WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5857 - accuracy: 0.6765 - val_loss: 1.5552 - val_accuracy: 0.6408\n",
            "Epoch 56/40000\n",
            "134/138 [============================>.] - ETA: 0s - loss: 0.5880 - accuracy: 0.6760WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.6762 - val_loss: 1.5542 - val_accuracy: 0.6394\n",
            "Epoch 57/40000\n",
            "123/138 [=========================>....] - ETA: 0s - loss: 0.5848 - accuracy: 0.6764WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.5844 - accuracy: 0.6780 - val_loss: 1.5520 - val_accuracy: 0.6362\n",
            "Epoch 58/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5811 - accuracy: 0.6842WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5826 - accuracy: 0.6835 - val_loss: 1.5681 - val_accuracy: 0.6367\n",
            "Epoch 59/40000\n",
            "129/138 [===========================>..] - ETA: 0s - loss: 0.5815 - accuracy: 0.6824WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5819 - accuracy: 0.6814 - val_loss: 1.5640 - val_accuracy: 0.6364\n",
            "Epoch 60/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5797 - accuracy: 0.6835WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5806 - accuracy: 0.6831 - val_loss: 1.5655 - val_accuracy: 0.6379\n",
            "Epoch 61/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.5794 - accuracy: 0.6823WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5789 - accuracy: 0.6829 - val_loss: 1.5701 - val_accuracy: 0.6377\n",
            "Epoch 62/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.5818 - accuracy: 0.6796WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.6797 - val_loss: 1.5577 - val_accuracy: 0.6343\n",
            "Epoch 63/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5767 - accuracy: 0.6837WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5774 - accuracy: 0.6832 - val_loss: 1.5604 - val_accuracy: 0.6379\n",
            "Epoch 64/40000\n",
            "136/138 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.6839WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.6839 - val_loss: 1.5851 - val_accuracy: 0.6400\n",
            "Epoch 65/40000\n",
            "131/138 [===========================>..] - ETA: 0s - loss: 0.5769 - accuracy: 0.6821WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5768 - accuracy: 0.6814 - val_loss: 1.5656 - val_accuracy: 0.6340\n",
            "Epoch 66/40000\n",
            "135/138 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.6848WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5756 - accuracy: 0.6844 - val_loss: 1.5684 - val_accuracy: 0.6354\n",
            "Epoch 67/40000\n",
            "132/138 [===========================>..] - ETA: 0s - loss: 0.5783 - accuracy: 0.6798WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5777 - accuracy: 0.6797 - val_loss: 1.5666 - val_accuracy: 0.6386\n",
            "Epoch 68/40000\n",
            "130/138 [===========================>..] - ETA: 0s - loss: 0.5762 - accuracy: 0.6826WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.6832 - val_loss: 1.5724 - val_accuracy: 0.6369\n",
            "Epoch 69/40000\n",
            "133/138 [===========================>..] - ETA: 0s - loss: 0.5732 - accuracy: 0.6881WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.5736 - accuracy: 0.6878 - val_loss: 1.5711 - val_accuracy: 0.6381\n",
            "Epoch 00069: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCG_BLhn6j18",
        "colab_type": "code",
        "outputId": "63c76237-7315-4d34-e5c6-caec61a709e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "model.save('gdrive/My Drive/sbgan/binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: gdrive/My Drive/sbgan/binary/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtFeA2ddruf",
        "colab_type": "text"
      },
      "source": [
        "# Analyze Profitability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7sSbfWDduVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Kelly: {np.average(profit.kelly)}\\n\")\n",
        "print(f\"Fixed: {np.average(profit.fixed)}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7JCumv-6uI8",
        "colab_type": "text"
      },
      "source": [
        "# Predict Games\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T709gUDEBkWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_wagers(games, odds, model):\n",
        "  keys = games['LINE']\n",
        "  games = games.drop(['WINS_WINNER', 'LOSSES_WINNER', 'WINS_LOSER', 'LOSSES_LOSER', 'LINE', 'ODDS'], axis=1)\n",
        "  scores = model.predict(games)\n",
        "\n",
        "  data = zip(keys, scores, odds)\n",
        "\n",
        "  predictions = {}\n",
        "\n",
        "  for k, p, bprime in data:\n",
        "    b = bprime - 1\n",
        "    q = 1 - p\n",
        "    units = round(float((((b*p) - q) / b) * 15))\n",
        "    predictions[k] = max(0, units)\n",
        "    \n",
        "  return predictions\n",
        "\n",
        "def predict_winners(games, model):\n",
        "  keys = games['LINE']\n",
        "  games = games.drop(['WINS_WINNER', 'LOSSES_WINNER', 'WINS_LOSER', 'LOSSES_LOSER', 'LINE', 'ODDS'], axis=1)\n",
        "  scores = model.predict(games)\n",
        "\n",
        "  data = zip(keys, scores)\n",
        "\n",
        "  predictions = {}\n",
        "\n",
        "  for k, p in data:\n",
        "    predictions[k] = p\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II11efo96vrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from google.colab import files\n",
        "model = load_model('gdrive/My Drive/sbgan/models/binary.h5')\n",
        "games = pd.read_csv('gdrive/My Drive/sbgan/nightly/24FEB.csv', index_col=0)\n",
        "\n",
        "odds = games['ODDS']\n",
        "\n",
        "winners = predict_winners(games, model)\n",
        "wagers = predict_wagers(games, odds, model)\n",
        "\n",
        "print(winners)\n",
        "print(wagers)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}